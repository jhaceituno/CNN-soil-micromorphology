{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp_test.ipynb","provenance":[{"file_id":"1JX7eugw4Mi6j0C8ac13YXYLoIFndrVvS","timestamp":1580743504545}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jT_FXG5p3tzM"},"source":["%tensorflow_version 1.x\n","\n","from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout\n","\n","from google.colab import drive\n","from math import ceil\n","import matplotlib.pyplot as plt\n","import datetime\n","import numpy as np\n","import os\n","\n","# Experiment configuration\n","USING_SPLITS = False\n","current_folder = '/content/gdrive/My Drive/public/sediments/'\n","\n","samples = 'splits' if USING_SPLITS else 'images'\n","drive.mount('/content/gdrive')\n","!cp \"{current_folder}{samples}.tar.gz\" /tmp\n","!tar xzvf /tmp/{samples}.tar.gz\n","BASE = '/content/{}/folds'.format(samples)\n","ALL_TRAIN_SET_PATH = '/content/splits/train' if USING_SPLITS else '/content/images/training_set'\n","TEST_SET_PATH = '/content/splits/test' if USING_SPLITS else '/content/images/test_set'\n","BASE_RESULTS = current_folder + 'results/' + samples\n","\n","CLASSES = ['background', 'chambers', 'channels', 'packing voids', 'planes', 'vesicles', 'vughs']\n","\n","results_path = BASE_RESULTS + '/MLP'\n","os.makedirs(results_path, exist_ok=True)\n","\n","# Network parameters\n","NUM_FOLDS = 1\n","input_size = 224\n","BATCH_SIZE = 32\n","EPOCHS = 80\n","\n","all_acc_histories = []\n","# For loop repeating the training for each fold: fine-tuning freezing the base model\n","for fold in range(1, NUM_FOLDS + 1):\n","    fold_path = BASE + '/fold_' + str(fold)\n","    train_path = fold_path + '/training_set'\n","    val_path = fold_path + '/validation_set'\n","    results_file = open(results_path + '/info.txt', 'w' if fold == 1 else 'a') \n","    \n","    hidden_units = 1000\n","    num_labels = 7\n","    model = Sequential()\n","    model.add(Dense(hidden_units, input_dim=input_size*input_size*3))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(hidden_units))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(hidden_units))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(hidden_units))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(hidden_units))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(num_labels))\n","    # this is the output for one-hot vector\n","    model.add(Activation('softmax'))\n","    model.summary()\n","    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","    \n","    train_batches = ImageDataGenerator(\n","        rescale=1. / 255, rotation_range=30, vertical_flip=True, horizontal_flip=True, zoom_range=[0.8, 1.2]\n","        ).flow_from_directory(train_path, target_size=(input_size, input_size), batch_size=BATCH_SIZE, classes=CLASSES)\n","    val_batches = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n","        val_path, target_size=(input_size, input_size), batch_size=BATCH_SIZE,classes=CLASSES)\n","    \n","    STEPS_EPOCH_TRAIN=train_batches.n//train_batches.batch_size\n","    STEPS_EPOCH_VALID=val_batches.n//val_batches.batch_size\n","  \n","    for e in range(EPOCHS):\n","        print('Epoch {}'.format(e))\n","        step = 0\n","        for x_batch, y_batch in train_batches:\n","            x_batch = np.reshape(x_batch, [-1, input_size * input_size * 3])\n","            model.fit(x_batch, y_batch, verbose=0)\n","            step += 1\n","            if (step >= STEPS_EPOCH_TRAIN):\n","                break\n","\n","      acc = []\n","      step = 0\n","      for x_valid, y_valid in val_batches:\n","         x_valid = np.reshape(x_valid, [-1, input_size * input_size * 3])\n","        scores = model.evaluate(x_valid, y_valid, batch_size=BATCH_SIZE, verbose=False)\n","        step += 1\n","        acc.append(scores[1])\n","        if (step >= STEPS_EPOCH_VALID):\n","          break\n","\n","      print('val acc: {}'.format(sum(acc) / len(acc)))\n","\n","test_batches = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n","    TEST_SET_PATH, target_size=(input_size, input_size), batch_size=BATCH_SIZE, classes=CLASSES)\n","STEPS_EPOCH_TEST = test_batches.n // test_batches.batch_size\n","acc = []\n","step = 0\n","for x_test, y_test in test_batches:\n","  x_test = np.reshape(x_test, [-1, input_size * input_size * 3])\n","  scores = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=False)\n","  step += 1\n","  print('Step {0}/{1}'.format(step, STEPS_EPOCH_TEST))\n","  acc.append(scores[1])\n","  if (step >= STEPS_EPOCH_TEST):\n","    break\n","\n","model.summary()"],"execution_count":null,"outputs":[]}]}